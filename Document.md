# Behavior Prediction from Everyday Sounds via LLMs with Multi-sensor Context and Priors

## Abstract
Sounds are ubiquitous in our daily life, and serve as crucial perceptual cues for understanding behavior and environment. From the rustling wind during a run to the clinking of cutlery at meals and the ambient chatter in an office, humans can infer the surrounding context and ongoing activities from sound, given appropriate context and prior knowledge. However, due to the inherent ambiguity of acoustic signals and the complexity of real-world environments, machines have long struggled with such inferential capabilities. With the recent advancements in large language models (LLMs) capable of multimodal processing, bridging this gap is now becoming feasible.
This study proposes a behavior recognition framework that integrates wearable devices with LLMs, aiming to achieve fine-grained user behavior prediction from audio data in everyday settings. We use audio collected from smartwatches as the primary modality, and enrich it with contextual signals from multiple sensors, including IMU (for motion state), GPS (for location), and physiological signals (e.g., heart rate). In addition, we incorporate user-specific prior knowledge such as demographic profiles, habitual routines, and preferences, forming structured auxiliary knowledge to support LLM-based reasoning.
Our core hypothesis is that multimodal fusion can effectively disambiguate acoustically similar activities, enabling higher-level behavioral inference. For instance, similar footsteps in audio may correspond to either walking to class or strolling in a mall, but GPS data and movement patterns help distinguish the two. Likewise, distinguishing between “selecting food” and “making a payment” may depend on personalized information such as whether the user has a meal plan. We encode these multimodal inputs into structured prompts and combine them with audio descriptions to feed into the LLM for predicting the user’s current activity and state.
Eventually, we expect the system to generate comprehensive daily activity summaries and behavioral reports, offering interpretable and adaptive feedback to users, supporting habit tracking, self-reflection, and future human–AI interactions.